{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "\n",
    "    - This script aims to predict real estate prices per square meter in Paris for the year 2022. It involves data preprocessing, feature engineering, model selection, hyperparameter tuning, and training the best model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "csv_file_path = 'transactions.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Check if the column 'Unnamed: 0' exists and drop it\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "# Check for empty columns\n",
    "empty_cols = df.columns[df.isnull().all()]\n",
    "if not empty_cols.empty:\n",
    "    # Remove empty columns\n",
    "    df = df.drop(empty_cols, axis=1)\n",
    "\n",
    "# Calculate price per square meter\n",
    "df['prix_m2'] = df['prix'] / df['surface_habitable']\n",
    "\n",
    "# List of department numbers for ÃŽle-de-France\n",
    "ile_de_france_departments = [75, 77, 78, 91, 92, 93, 94, 95]\n",
    "\n",
    "# Filter for transactions in Paris in 2022\n",
    "paris_df = df[(df.departement.isin(ile_de_france_departments)) & (df.date_transaction.str.startswith('2022-'))].copy()\n",
    "\n",
    "# Process surface columns\n",
    "surface_cols = [c for c in paris_df.columns if 'surface_' in c and c != 'surface_habitable']\n",
    "for c in surface_cols:\n",
    "    paris_df[c + '_sum'] = paris_df[c].apply(lambda x: sum(eval(x)) if 'NULL' not in x else 0)\n",
    "    \n",
    "# Filter rows where the sum of surface columns is 0\n",
    "paris_df = paris_df[paris_df[[c + '_sum' for c in surface_cols]].sum(axis=1) == 0]\n",
    "\n",
    "# Convert date_transaction to datetime and calculate days since epoch\n",
    "paris_df['date_transaction'] = pd.to_datetime(paris_df['date_transaction'])\n",
    "reference_date = pd.to_datetime('1970-01-01')\n",
    "paris_df['days_since_epoch'] = (paris_df['date_transaction'] - reference_date).dt.days\n",
    "\n",
    "# Drop the original date_transaction column\n",
    "paris_df = paris_df.drop('date_transaction', axis=1)\n",
    "\n",
    "# Create dummy variables for type_batiment\n",
    "paris_df = pd.get_dummies(paris_df, columns=['type_batiment'], prefix='type')\n",
    "\n",
    "# Select relevant features for X and y\n",
    "X = paris_df[['days_since_epoch', 'longitude', 'latitude', 'id_ville', 'type_Appartement', 'type_Maison', 'vefa', 'n_pieces', 'surface_habitable']]\n",
    "y = paris_df['prix_m2']\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Save the processed DataFrame to a new file\n",
    "dataset_file_path = 'dataset.csv'\n",
    "paris_df.to_csv(dataset_file_path, index=False)\n",
    "\n",
    "dataset_file_path\n",
    "\n",
    "# Division of the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid of parameters for the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid = {\n",
    "    'DTR': {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'params': {\n",
    "            'max_depth': range(1, 101, 10),\n",
    "            'min_samples_split': range(2, 21, 2)\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'n_neighbors': range(1, 51, 5)\n",
    "        }\n",
    "    },\n",
    "    'LR': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RFR': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'max_depth': range(71, 89, 4),\n",
    "            'min_samples_leaf': range(11, 13, 1),\n",
    "            'n_estimators': [600, 800, 850, 900]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of models with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.01)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# Iterate over each model configuration\n",
    "for model_name, model_config in tqdm(params_grid.items()):\n",
    "    # Setting up GridSearchCV with verbosity\n",
    "    gs = GridSearchCV(estimator=model_config['model'], param_grid=model_config['params'], verbose=3, n_jobs=-1)\n",
    "\n",
    "    # Fitting the model and plotting learning curve in real-time\n",
    "    gs.fit(X_train, y_train)\n",
    "    plot_learning_curve(gs.best_estimator_, X_train, y_train, title=f\"Learning Curve for {model_name}\", cv=5, n_jobs=-1)\n",
    "\n",
    "    # Extracting the best model, its parameters, and performance metrics\n",
    "    best_model = gs.best_estimator_\n",
    "    best_params = gs.best_params_\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, best_model.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, best_model.predict(X_test)))\n",
    "    score = best_model.score(X_test, y_test)\n",
    "\n",
    "    # Saving the best model\n",
    "    best_models[model_name] = {\n",
    "        'model': best_model,\n",
    "        'params': best_params,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "    # Display of results for each model\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Optimal params: {best_params}\")\n",
    "    print(f\"Train RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    print(f\"Model Score: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_name = max(best_models, key=lambda k: best_models[k]['score'])\n",
    "best_model_to_train = best_models[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nAutomatically training the best found model ({best_model_name}) with the optimal parameters...\")\n",
    "best_model_to_train.fit(X, y)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling the best model\n",
    "pickle_file_path = 'best_model.pkl'\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(best_model_to_train, file)\n",
    "\n",
    "print(f\"Model saved as {pickle_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
